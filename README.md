# NeuroVoice ğŸ§ ğŸ”Š

**A Multimodal AI System for Diagnosing Alzheimer's, Parkinson's, and Depression from Speech and Facial Expression Data**

NeuroVoice is a deep learning framework that leverages **multimodal fusion** (audio + facial expressions) to detect neurodegenerative and mental health conditions. The system uses state-of-the-art transformer embeddings and cross-modal attention mechanisms to identify biomarkers in speech patterns and facial microexpressions.

---

## ğŸ¯ Features

- **Multimodal Fusion**: Combines audio and visual data for improved diagnosis accuracy
- **Transformer-Based Embeddings**: Uses wav2vec2 for speech and Vision Transformers for facial expressions
- **Cross-Modal Attention**: Quantifies correlation between facial microexpressions and vocal tremors
- **Explainability Layer**: Visualizes neurodegenerative biomarkers in speech and facial dynamics
- **Multi-Disease Classification**: Supports Alzheimer's, Parkinson's, and Depression detection

---

## ğŸ“¦ Installation

### Prerequisites

- Python 3.10+
- CUDA-capable GPU (optional, CPU fallback available)

### Setup

1. Clone the repository:
```bash
git clone https://github.com/yourusername/NeuroVoice.git
cd NeuroVoice
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

---

## ğŸ“Š Datasets

NeuroVoice uses the following publicly available datasets:

### 1. **DementiaBank Pitt Corpus** (Alzheimer's)
- **Source**: [https://dementia.talkbank.org/access/English/Pitt.html](https://dementia.talkbank.org/access/English/Pitt.html)
- Contains speech recordings and transcripts from Alzheimer's patients and controls
- Requires registration and agreement to terms of use

### 2. **DAIC-WOZ** (Depression)
- **Source**: [https://dcapswoz.ict.usc.edu/](https://dcapswoz.ict.usc.edu/)
- Contains audio, video, and transcript data from clinical interviews
- Free for academic use (requires signup)

### 3. **Parkinson's Telemonitoring Dataset** (UCI ML Repository)
- **Source**: [https://archive.ics.uci.edu/ml/datasets/parkinsons](https://archive.ics.uci.edu/ml/datasets/parkinsons)
- Contains 195 sustained phonations from Parkinson's patients and controls
- Publicly available

### 4. **FER2013** (Facial Expressions)
- **Source**: [https://www.kaggle.com/datasets/msambare/fer2013](https://www.kaggle.com/datasets/msambare/fer2013)
- Large-scale facial expression dataset with 7 emotion classes
- Available via Kaggle API

### Downloading Datasets

Run the automated download script:

```bash
python scripts/download_data.py
```

**Note**: Some datasets require manual registration. The script will provide instructions for datasets that cannot be automatically downloaded.

---

## ğŸš€ Quick Start

### 1. Data Preparation

Download and preprocess the datasets:

```bash
# Download datasets
python scripts/download_data.py

# Preprocess audio data
python scripts/preprocess_audio.py --dataset all

# Preprocess video data
python scripts/preprocess_video.py --dataset all

# Split data into train/val/test
python scripts/split_data.py --split 0.7 0.15 0.15
```

### 2. Training

Train the multimodal fusion model:

```bash
python src/training/train.py \
    --disease alzheimer \
    --epochs 50 \
    --batch_size 16 \
    --lr 1e-4 \
    --use_gpu
```

Train for multiple diseases:

```bash
# Alzheimer's
python src/training/train.py --disease alzheimer --epochs 50

# Parkinson's
python src/training/train.py --disease parkinson --epochs 50

# Depression
python src/training/train.py --disease depression --epochs 50
```

### 3. Evaluation

Evaluate a trained model:

```bash
python src/training/evaluate.py \
    --model_path outputs/models/best_model.pt \
    --disease alzheimer \
    --test_data data/processed/test/
```

### 4. Jupyter Notebooks

Explore the analysis notebooks:

```bash
jupyter notebook notebooks/
```

- `01_data_exploration.ipynb` - Dataset overview and statistics
- `02_audio_feature_analysis.ipynb` - Audio feature extraction and visualization
- `03_video_feature_analysis.ipynb` - Facial expression analysis
- `04_multimodal_training.ipynb` - Interactive training and experimentation
- `05_results_visualization.ipynb` - Model performance metrics and plots
- `06_explainability.ipynb` - Attention maps and saliency visualization

---

## ğŸ—ï¸ Project Structure

```
NeuroVoice/
â”‚
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”œâ”€â”€ LICENSE                      # MIT License
â”‚
â”œâ”€â”€ data/                        # Dataset storage
â”‚   â”œâ”€â”€ README.md                # Dataset documentation
â”‚   â”œâ”€â”€ daic_woz/                # DAIC-WOZ depression dataset
â”‚   â”œâ”€â”€ parkinson_tsi/           # Parkinson's dataset
â”‚   â”œâ”€â”€ dementiabank/            # Alzheimer's speech dataset
â”‚   â””â”€â”€ faces/                   # Facial expression datasets
â”‚
â”œâ”€â”€ scripts/                     # Utility scripts
â”‚   â”œâ”€â”€ download_data.py         # Automated data downloaders
â”‚   â”œâ”€â”€ preprocess_audio.py      # Audio preprocessing
â”‚   â”œâ”€â”€ preprocess_video.py      # Video preprocessing
â”‚   â””â”€â”€ split_data.py            # Data splitting logic
â”‚
â”œâ”€â”€ src/                         # Source code
â”‚   â”œâ”€â”€ config.py                # Configuration and paths
â”‚   â”œâ”€â”€ data_loaders/            # PyTorch data loaders
â”‚   â”œâ”€â”€ features/                # Feature extraction modules
â”‚   â”œâ”€â”€ models/                  # Deep learning models
â”‚   â”œâ”€â”€ training/                # Training and evaluation
â”‚   â””â”€â”€ utils/                   # Utility functions
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_audio_feature_analysis.ipynb
â”‚   â”œâ”€â”€ 03_video_feature_analysis.ipynb
â”‚   â”œâ”€â”€ 04_multimodal_training.ipynb
â”‚   â”œâ”€â”€ 05_results_visualization.ipynb
â”‚   â””â”€â”€ 06_explainability.ipynb
â”‚
â”œâ”€â”€ tests/                       # Unit tests
â”‚   â”œâ”€â”€ test_audio_pipeline.py
â”‚   â”œâ”€â”€ test_video_pipeline.py
â”‚   â”œâ”€â”€ test_model_training.py
â”‚   â””â”€â”€ test_fusion_model.py
â”‚
â””â”€â”€ outputs/                     # Training outputs
    â”œâ”€â”€ models/                  # Saved model checkpoints
    â”œâ”€â”€ logs/                    # Training logs
    â”œâ”€â”€ metrics/                 # Evaluation metrics
    â””â”€â”€ visualizations/          # Generated plots
```

---

## ğŸ§  Model Architecture

### Audio Processing
- **MFCC Features**: 13-dimensional MFCCs extracted from speech segments
- **Wav2Vec2 Embeddings**: Transformer-based speech embeddings from `facebook/wav2vec2-base-960h`
- **LSTM/CNN Encoder**: Temporal modeling of audio sequences

### Video Processing
- **MediaPipe Landmarks**: 468 facial landmarks for geometric features
- **Emotion Embeddings**: Pre-trained emotion classifier (trained on FER2013/AffectNet)
- **ResNet/ViT Encoder**: Deep feature extraction from facial frames

### Multimodal Fusion
- **Cross-Modal Attention**: Learns interactions between audio and visual modalities
- **Gated Fusion**: Adaptive weighting of modalities
- **Classification Head**: Multi-task learning for disease classification

---

## ğŸ“ˆ Results

### Example Metrics (Placeholder - Update after training)

| Disease | Accuracy | ROC-AUC | Sensitivity | Specificity |
|---------|----------|---------|-------------|-------------|
| Alzheimer's | - | - | - | - |
| Parkinson's | - | - | - | - |
| Depression | - | - | - | - |

---

## ğŸ§ª Testing

Run the test suite:

```bash
pytest tests/
```

Or run individual tests:

```bash
pytest tests/test_audio_pipeline.py
pytest tests/test_fusion_model.py
```

---

## ğŸ”¬ Explainability

Visualize model attention and saliency maps:

```bash
jupyter notebook notebooks/06_explainability.ipynb
```

The explainability module provides:
- **Attention Heatmaps**: Shows which audio segments and facial regions the model focuses on
- **Saliency Maps**: Highlights important features in waveforms and video frames
- **Gradient Visualization**: Grad-CAM style visualizations for interpretability

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“ Citation

If you use NeuroVoice in your research, please cite:

```bibtex
@software{neurovoice2024,
  title={NeuroVoice: Multimodal AI for Neurodegenerative Disease Detection},
  author={Your Name},
  year={2024},
  url={https://github.com/yourusername/NeuroVoice}
}
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **DementiaBank** for providing the Alzheimer's speech corpus
- **USC Institute for Creative Technologies** for the DAIC-WOZ dataset
- **UCI Machine Learning Repository** for the Parkinson's dataset
- **Kaggle** for hosting the FER2013 dataset
- **Hugging Face** for transformer models (wav2vec2)
- **MediaPipe** for facial landmark detection

---

## âš ï¸ Disclaimer

**This tool is for research purposes only and should not be used for clinical diagnosis without proper validation and medical supervision.**

---

## ğŸ“§ Contact

For questions or issues, please open an issue on GitHub or contact [your-email@example.com].

---

**Built with â¤ï¸ for advancing healthcare AI**

